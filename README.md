RAG Evaluation Tool

Metric-Based Scoring: Uses Gemini 1.5 Flash as an "LLM-as-a-Judge" to score a RAG model on Hallucination, Relevance, and Accuracy via LangChain Hub prompts.

Dataset Automation: Enables users to upload ground truth CSVs and automatically creates structured, traceable LangSmith Datasets.

Experiment Execution: Runs the RAG application across the dataset using langsmith.evaluate to automate the entire testing pipeline.

Traceability & Reporting: Centralizes all results and execution traces on LangSmith, providing direct links for detailed performance analysis.
